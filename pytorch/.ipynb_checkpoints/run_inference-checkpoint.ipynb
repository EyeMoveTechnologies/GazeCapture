{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, shutil, os, time, argparse\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "from ITrackerData import ITrackerData\n",
    "from ITrackerModel import ITrackerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = 16\n",
    "epochs = 25\n",
    "batch_size = torch.cuda.device_count()*100 # Change if out of cuda memory\n",
    "\n",
    "base_lr = 0.0001\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "print_freq = 10\n",
    "prec1 = 0\n",
    "best_prec1 = 1e20\n",
    "lr = base_lr\n",
    "\n",
    "count_test = 0\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion,optimizer, epoch):\n",
    "    global count\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    for i, (row, imFace, imEyeL, imEyeR, faceGrid, gaze) in enumerate(train_loader):\n",
    "        \n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        imFace = imFace.cuda()\n",
    "        imEyeL = imEyeL.cuda()\n",
    "        imEyeR = imEyeR.cuda()\n",
    "        faceGrid = faceGrid.cuda()\n",
    "        gaze = gaze.cuda()\n",
    "        \n",
    "        imFace = torch.autograd.Variable(imFace, requires_grad = True)\n",
    "        imEyeL = torch.autograd.Variable(imEyeL, requires_grad = True)\n",
    "        imEyeR = torch.autograd.Variable(imEyeR, requires_grad = True)\n",
    "        faceGrid = torch.autograd.Variable(faceGrid, requires_grad = True)\n",
    "        gaze = torch.autograd.Variable(gaze, requires_grad = False)\n",
    "\n",
    "        # compute output\n",
    "        output = model(imFace, imEyeL, imEyeR, faceGrid)\n",
    "\n",
    "        loss = criterion(output, gaze)\n",
    "        \n",
    "        losses.update(loss.data.item(), imFace.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        count=count+1\n",
    "\n",
    "        print('Epoch (train): [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses))\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch):\n",
    "    global count_test\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    lossesLin = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "    oIndex = 0\n",
    "    for i, (row, imFace, imEyeL, imEyeR, faceGrid, gaze) in enumerate(val_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        imFace = imFace.cuda()\n",
    "        imEyeL = imEyeL.cuda()\n",
    "        imEyeR = imEyeR.cuda()\n",
    "        faceGrid = faceGrid.cuda()\n",
    "        gaze = gaze.cuda()\n",
    "        \n",
    "        imFace = torch.autograd.Variable(imFace, requires_grad = False)\n",
    "        imEyeL = torch.autograd.Variable(imEyeL, requires_grad = False)\n",
    "        imEyeR = torch.autograd.Variable(imEyeR, requires_grad = False)\n",
    "        faceGrid = torch.autograd.Variable(faceGrid, requires_grad = False)\n",
    "        gaze = torch.autograd.Variable(gaze, requires_grad = False)\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            output = model(imFace, imEyeL, imEyeR, faceGrid)\n",
    "\n",
    "        loss = criterion(output, gaze)\n",
    "        \n",
    "        lossLin = output - gaze\n",
    "        lossLin = torch.mul(lossLin,lossLin)\n",
    "        lossLin = torch.sum(lossLin,1)\n",
    "        lossLin = torch.mean(torch.sqrt(lossLin))\n",
    "\n",
    "        losses.update(loss.data.item(), imFace.size(0))\n",
    "        lossesLin.update(lossLin.item(), imFace.size(0))\n",
    "     \n",
    "        # compute gradient and do SGD step\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        print('Epoch (val): [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Error L2 {lossLin.val:.4f} ({lossLin.avg:.4f})\\t'.format(\n",
    "                    epoch, i, len(val_loader), batch_time=batch_time,\n",
    "                   loss=losses,lossLin=lossesLin))\n",
    "\n",
    "    return lossesLin.avg\n",
    "\n",
    "CHECKPOINTS_PATH = '.'\n",
    "\n",
    "def load_checkpoint(filename='checkpoint.pth.tar'):\n",
    "    filename = os.path.join(CHECKPOINTS_PATH, filename)\n",
    "    print(filename)\n",
    "    if not os.path.isfile(filename):\n",
    "        return None\n",
    "    state = torch.load(filename)\n",
    "    return state\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    if not os.path.isdir(CHECKPOINTS_PATH):\n",
    "        os.makedirs(CHECKPOINTS_PATH, 0o777)\n",
    "    bestFilename = os.path.join(CHECKPOINTS_PATH, 'best_' + filename)\n",
    "    filename = os.path.join(CHECKPOINTS_PATH, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, bestFilename)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = base_lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.state_dict()['param_groups']:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "global args, best_prec1, weight_decay, momentum\n",
    "\n",
    "model = ITrackerModel()\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.cuda()\n",
    "imSize=(224,224)\n",
    "cudnn.benchmark = True   \n",
    "\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "doLoad = True\n",
    "doTest = True\n",
    "data_path = '../../datasets/prepared'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoint.pth.tar\n",
      "Loading checkpoint for epoch 00025 with loss 2.43143 (which is the mean squared error not the actual linear error)...\n"
     ]
    }
   ],
   "source": [
    "if doLoad:\n",
    "    saved = load_checkpoint()\n",
    "    if saved:\n",
    "        print('Loading checkpoint for epoch %05d with loss %.5f (which is the mean squared error not the actual linear error)...' % (saved['epoch'], saved['best_prec1']))\n",
    "        state = saved['state_dict']\n",
    "        try:\n",
    "            model.module.load_state_dict(state)\n",
    "        except:\n",
    "            model.load_state_dict(state)\n",
    "        epoch = saved['epoch']\n",
    "        best_prec1 = saved['best_prec1']\n",
    "    else:\n",
    "        print('Warning: Could not read checkpoint!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading iTracker dataset...\n",
      "\tReading metadata from ../../datasets/prepared/metadata.mat...\n",
      "\tReading metadata from ./mean_face_224.mat...\n",
      "\tReading metadata from ./mean_left_224.mat...\n",
      "\tReading metadata from ./mean_right_224.mat...\n",
      "Loaded iTracker dataset split \"train\" with 1251983 records...\n",
      "Loading iTracker dataset...\n",
      "\tReading metadata from ../../datasets/prepared/metadata.mat...\n",
      "\tReading metadata from ./mean_face_224.mat...\n",
      "\tReading metadata from ./mean_left_224.mat...\n",
      "\tReading metadata from ./mean_right_224.mat...\n",
      "Loaded iTracker dataset split \"test\" with 179496 records...\n"
     ]
    }
   ],
   "source": [
    "dataTrain = ITrackerData(dataPath = data_path, split='train', imSize = imSize)\n",
    "dataVal = ITrackerData(dataPath = data_path, split='test', imSize = imSize)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataTrain,\n",
    "    batch_size=batch_size, shuffle=True,\n",
    "    num_workers=workers, pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataVal,\n",
    "    batch_size=batch_size, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss().cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr,\n",
    "                            momentum=momentum,\n",
    "                            weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "if doTest:\n",
    "    validate(val_loader, model, criterion, epoch)\n",
    "else:\n",
    "    for epoch in range(0, epoch):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    for epoch in range(epoch, epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, model, criterion, epoch)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 < best_prec1\n",
    "        best_prec1 = min(prec1, best_prec1)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "        }, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ITrackerData.ITrackerData object at 0x7fafa4c650f0>\n"
     ]
    }
   ],
   "source": [
    "for i, (row, imFace, imEyeL, imEyeR, faceGrid, gaze) in enumerate(val_loader):\n",
    "    print(i)\n",
    "    print(imFace.shape)\n",
    "    print(imEyeL.shape)\n",
    "    print(imEyeR.shape)\n",
    "    print(faceGrid.shape)\n",
    "    print(gaze.shape)\n",
    "    break\n",
    "print(dataVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
